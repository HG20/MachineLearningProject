import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder

# Load dataset
data_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
columns = [
    "age", "workclass", "fnlwgt", "education", "education-num", "marital-status",
    "occupation", "relationship", "race", "sex", "capital-gain", "capital-loss",
    "hours-per-week", "native-country", "income"
]

data = pd.read_csv(data_url, header=None, names=columns, na_values=" ?", skipinitialspace=True)

# Display basic information about the dataset
data.info()
display(data.head())

# 1. Handling Missing Values
data.dropna(inplace=True)  # Drop rows with missing values

# 2. Removing Duplicates
data.drop_duplicates(inplace=True)

# 3. Encoding Categorical Variables
categorical_cols = ["workclass", "education", "marital-status", "occupation", "relationship", "race", "sex", "native-country", "income"]
encoder = OneHotEncoder(drop='first', sparse=False)
categorical_encoded = pd.DataFrame(encoder.fit_transform(data[categorical_cols]))
categorical_encoded.columns = encoder.get_feature_names_out(categorical_cols)

# Drop original categorical columns and replace with encoded ones
data = data.drop(columns=categorical_cols).reset_index(drop=True)
data = pd.concat([data, categorical_encoded], axis=1)

# 4. Outlier Detection & Treatment (Capping extreme values)
numeric_cols = ["age", "fnlwgt", "education-num", "capital-gain", "capital-loss", "hours-per-week"]
for col in numeric_cols:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    data[col] = np.clip(data[col], lower_bound, upper_bound)

# 5. Normalization / Standardization
scaler = StandardScaler()
data[numeric_cols] = scaler.fit_transform(data[numeric_cols])

# 6. Train-Test Split
X = data.drop(columns=["hours-per-week"])  # Features
y = data["hours-per-week"]  # Target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 7. Save Preprocessed Data
train_data = pd.concat([X_train, y_train], axis=1)
test_data = pd.concat([X_test, y_test], axis=1)
train_data.to_csv("train_data.csv", index=False)
test_data.to_csv("test_data.csv", index=False)

print("Preprocessing complete. Train and test datasets saved.")
